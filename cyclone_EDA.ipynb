{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this code to read and transform data if we begin with initial combined dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100000  # Process 100,000 rows at a time\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    r\"C:\\Users\\user\\Desktop\\25 WI\\Combined_WWLLN_Locations.txt\", \n",
    "    delim_whitespace=True, \n",
    "    chunksize=chunksize\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "locations_WWLLN = pd.concat(chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\Combined_Reduced_Trackfile.txt\", sep=\"\\t\")\n",
    "track_file = track_file.drop(track_file.columns[8], axis=1)\n",
    "print(track_file.head(5))\n",
    "# print(locations_WWLLN.head(5))\n",
    "print(locations_WWLLN.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN.columns = ['year', 'month', 'day', 'hour', 'min', 'sec','lat','lon','distance_from_storm_center_km_east', 'distance_from_storm_center_km_north', 'storm_code','storm_name']  # Replace with actual names\n",
    "track_file.columns = ['year', 'month', 'day','hour','lat','lon','pressure', 'knots', 'storm_code', 'storm_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate hypothenuse using existing north and east distance from the storm center and create inner core indicator - if 1 if hypotenuse is equal or less than 100km else 0 (rainband)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN['hypotenuse_disance_from_storm_center'] = np.sqrt(locations_WWLLN['distance_from_storm_center_km_east'] ** 2 +locations_WWLLN['distance_from_storm_center_km_north'] ** 2)\n",
    "locations_WWLLN[\"inner_core_ind\"] = locations_WWLLN[\"hypotenuse_disance_from_storm_center\"].apply(\n",
    "    lambda x: 1 if x <= 100 else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(track_file.shape)\n",
    "print(locations_WWLLN.shape)\n",
    "print(track_file[track_file['year'] == 2015].shape)\n",
    "\n",
    "print(locations_WWLLN[locations_WWLLN['year'] == 2015].shape)\n",
    "print(locations_WWLLN[locations_WWLLN['year'] == 2016].shape)\n",
    "print(locations_WWLLN[locations_WWLLN['year'] == 2017].shape)\n",
    "print(locations_WWLLN[locations_WWLLN['year'] == 2018].shape)\n",
    "print(locations_WWLLN[locations_WWLLN['year'] == 2019].shape)\n",
    "print(locations_WWLLN[locations_WWLLN['year'] == 2020].shape)\n",
    "\n",
    "# print(locations_WWLLN[(locations_WWLLN['year'] == 2016) & (locations_WWLLN['month'] == 12)].shape)\n",
    "\n",
    "# Re-check the shape after the transformation\n",
    "print(track_file.head())\n",
    "print(locations_WWLLN.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#track_file_2015.to_csv('track_file_2015_2015.csv', index=False)  \n",
    "#locations_WWLLN_2015.to_csv('Combined_WWLLN_Locations_2015.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file_pl = pl.from_pandas(track_file)\n",
    "locations_WWLLN_pl = pl.from_pandas(locations_WWLLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file.to_csv('Combined_Reduced_Trackfile_w_header.csv', index=False)  \n",
    "locations_WWLLN_pl.write_csv('Combined_WWLLN_Locations_w_header.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploratory Data Analysis\n",
    "1) Number of unique storm per year in WWLLN and trackfile\n",
    "2) Number of lightning for each year & storm code\n",
    "3) Number of inner core lightning (3% of total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN.groupby('year')['storm_code'].nunique()\n",
    "track_file.groupby('year')['storm_code'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = locations_WWLLN.groupby(['year', 'storm_code']).size()\n",
    "testing = testing.reset_index(name='count')\n",
    "print(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'locations_WWLLN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlocations_WWLLN\u001b[49m[(locations_WWLLN[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner_core_ind\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'locations_WWLLN' is not defined"
     ]
    }
   ],
   "source": [
    "print(locations_WWLLN[(locations_WWLLN['inner_core_ind'] == 1)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this if we direclty read trasformed data set in polar format - which is in the google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_pl = pl.read_csv('Combined_WWLLN_Locations_w_header.txt', separator=\" \")  # For tab-separated values\n",
    "track_file_pl = pl.read_csv('Combined_Reduced_Trackfile_w_header.csv', separator=\" \")  # For tab-separated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function that joins two data set ...\n",
    "My kernel broke here btw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dates = locations_WWLLN_pl.join_asof(\n",
    "    track_file_pl,\n",
    "    on=\"hour\",\n",
    "    by=[\"year\", \"month\", \"day\", \"storm_code\", \"storm_name\"],  # Optional: match on multiple keys\n",
    "    strategy=\"backward\"  # \"backward\" (default) or \"forward\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_dates.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
