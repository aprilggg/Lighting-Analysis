{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Files - Creating and Cleaning Consolidated Data Files\n",
    "\n",
    "This notebook adds two necessary columns to our data files and combines the individual `.txt` files into two larger `.txt` files. Execution of this notebook will create `Combined_Reduced_Trackfile.txt` and `Combined_WWLLN_Locations.txt`. This notebook should be executed after the `data_file_cleaning.ipynb` notebook.\n",
    "\n",
    "We then perform some post-processing on the data by adding column headers, filtering to tropical cyclones that are category 1 or higher, and calculating the direct distance of each lightning strike to the TC storm center. This will create additional `Filtered_Reduced_Trackfile.csv` and `Filtered_WWLLN_Locations.txt` files for use in analysis. \n",
    "\n",
    "The last part of this notebook joins the trackfile and WWLLN locations together, where we bin the lightning data by 30 minute increments and join to the closest storm center timestamp to get the wind speed and pressure data. This portion will create the `some file` for use in analysis.\n",
    "\n",
    "### Combining Files\n",
    "We use the [Google Drive API](https://developers.google.com/drive/api/guides/about-sdk) to download the files previously uploaded in `data_upload.ipynb` to consolidate the individual files. The first half of the code works if the Google Drive API is already set up (refer to instructions in `data_upload.ipynb`). The code after we create the list of files is not dependent on the Google Drive API.\n",
    "\n",
    "Let's start by installing necessary packages and then importing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import os\n",
    "import polars as pl\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from io import BytesIO\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the function in `data_file_cleaning.ipynb`, we use the following function to authenticate the Google Drive API. This will open a browser to perform the authentication process. \n",
    "\n",
    "Check if a `token.pickle` file already exists before running the following code. If the file exists, it is recommended to delete it before running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=389849867563-4uggnm57nqe52156v32gj1lkosoqpoem.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A37635%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=GyfvJGpr2mtpuRzI6AP8xi3h1k3QIH&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "# Scopes for accessing Google Drive\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Authenticate and create the service object\n",
    "def authenticate_drive_api():\n",
    "    creds = None\n",
    "    # Token file for saving the authentication\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no credentials, perform authentication\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'client_secrets.json', SCOPES)  # Ensure 'credentials.json' is downloaded from Google API Console\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for future use\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Initialize the service object\n",
    "service = authenticate_drive_api()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function grabs the list of all files in a specified folder that are not trashed and stores them into a list. Each file has an ID and name attribute that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(folder_id):\n",
    "    # Query to find files in the specified folder\n",
    "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "    files = []\n",
    "\n",
    "    # List files in the folder and append to list\n",
    "    page_token = None\n",
    "    while True:\n",
    "        response = service.files().list(\n",
    "            q=query,\n",
    "            spaces='drive',\n",
    "            fields='nextPageToken, files(id, name)',\n",
    "            pageToken=page_token\n",
    "        ).execute()\n",
    "\n",
    "        files += response.get('files', [])\n",
    "\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function to find files in the specified folder. The folder ID can be found as the string after the \"folders/\" part of the URL for the Google Drive folder. This will give us a list of files to iterate through for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the folder\n",
    "folder_id = '14idmMBbM5xXZg4b61iINHbBTl2z4yLeD'\n",
    "files = find_files(folder_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split out the tropical cyclone ID and name from each of the files to add as a separate column. We then save the files in the `processed_files` directory for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each file to add cyclone ID and name as columns\n",
    "# Directory to save the processed files locally\n",
    "output_dir = \"processed_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    file_id = file['id']\n",
    "    file_name = file['name']\n",
    "\n",
    "    # Extract the cyclone ID and name from the filename\n",
    "    cyclone_id = '_'.join(file_name.split('_')[:3])\n",
    "    cyclone_name = file_name.split('_')[3]\n",
    "\n",
    "    # Download the file content\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    file_stream = BytesIO()\n",
    "    downloader = MediaIoBaseDownload(file_stream, request)\n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "    file_stream.seek(0)\n",
    "    content = file_stream.read().decode('utf-8')\n",
    "\n",
    "    # Add the cyclone id and name as a new column using Polars\n",
    "    df = pl.read_csv(BytesIO(content.encode('utf-8')),separator='\\t', has_header=False)\n",
    "    df = df.with_columns([\n",
    "    pl.lit(cyclone_id).alias(\"cyclone_id\"),\n",
    "    pl.lit(cyclone_name).alias(\"cyclone_name\")\n",
    "    ])\n",
    "\n",
    "    # Save the modified DataFrame locally\n",
    "    output_file_path = os.path.join(output_dir, file_name)\n",
    "    df.write_csv(output_file_path, separator='\\t',include_header=False)\n",
    "\n",
    "    print(f\"Processed and saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine each of the trackfiles in the `processed_files` folder into one file, and each of the WWLLN location files into one file. This will give us two output files in the `combined_files` folder - `Combined_Reduced_Trackfile.txt` and `Combined_WWLLN_Locations.txt`. We will use these files as the basis for our subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining 992 files for pattern 'Reduced_Trackfile'...\n",
      "Combined file saved: combined_files/Combined_Reduced_Trackfile.txt\n",
      "Combining 994 files for pattern 'WWLLN_Locations'...\n",
      "Combined file saved: combined_files/Combined_WWLLN_Locations.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Directories for processed files and output\n",
    "input_dir = \"processed_files\"\n",
    "output_dir = \"combined_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# File patterns to combine\n",
    "patterns = {\n",
    "    \"Reduced_Trackfile\": os.path.join(input_dir, \"*Reduced_Trackfile*.txt\"),\n",
    "    \"WWLLN_Locations\": os.path.join(input_dir, \"*WWLLN_Locations*.txt\")\n",
    "}\n",
    "\n",
    "# Combine files based on patterns\n",
    "for pattern_name, pattern_path in patterns.items():\n",
    "    combined_content = []\n",
    "    output_file_path = os.path.join(output_dir, f\"Combined_{pattern_name}.txt\")\n",
    "\n",
    "    # Find all matching files\n",
    "    matching_files = glob.glob(pattern_path)\n",
    "    print(f\"Combining {len(matching_files)} files for pattern '{pattern_name}'...\")\n",
    "\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for file_path in matching_files:\n",
    "            with open(file_path, \"r\") as input_file:\n",
    "                for line in input_file:\n",
    "                    output_file.write(line)\n",
    "\n",
    "    print(f\"Combined file saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Processing\n",
    "In this section we add a column header to the files and filter down to TCs that are category 1 and above. Category 1 is defined using the [Saffir-Simpson Hurricane Wind Scale](https://www.nhc.noaa.gov/aboutsshws.php), where the maximum sustained wind speed is between 64-82 kt. We calculate each TC's category using the Saffir-Simpson Scale and save it in a new column. We then calculate the direct distance of each lightning strike to the storm center and denote it as inner core or rainband. This section outputs `Filtered_Reduced_Trackfile.csv` and `Filtered_WWLLN_Locations.txt` files.\n",
    "\n",
    "Start by importing the necessary libraries and files created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import txt files created earlier\n",
    "# define the path to file below\n",
    "trackfile_path = \"Combined_Reduced_Trackfile.txt\"\n",
    "wwlln_path = \"Combined_WWLLN_Locations.txt\"\n",
    "\n",
    "track_file = pd.read_csv(trackfile_path, sep=\"\\t\")\n",
    "track_file = track_file.drop(track_file.columns[8], axis=1) # drop the column of zeros\n",
    "\n",
    "# !!!! commented out bc elaine's computer will die so this is for u janice <3\n",
    "# chunksize = 100000  # Process 100,000 rows at a time\n",
    "# chunks = []\n",
    "\n",
    "# for chunk in pd.read_csv(\n",
    "#     wwlln_path,\n",
    "#     delim_whitespace=True,\n",
    "#     chunksize=chunksize\n",
    "# ):\n",
    "#     chunks.append(chunk)\n",
    "\n",
    "# locations_WWLLN = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add headers to the two dataframes for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file.columns = ['year', 'month', 'day','hour','lat','lon','pressure', 'knots', 'storm_code', 'storm_name']\n",
    "#locations_WWLLN.columns = ['year', 'month', 'day', 'hour', 'min', 'sec','lat','lon','distance_from_storm_center_km_east', 'distance_from_storm_center_km_north', 'storm_code','storm_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we process the trackfile data by creating the list of storm codes that meet the category 1 or higher requirement. We will use this list to filter the wind speed/pressure data as well as the lightning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_code</th>\n",
       "      <th>max_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL_10_10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL_10_11</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL_10_12</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL_10_13</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  storm_code  max_wind_speed\n",
       "0   ATL_10_1              85\n",
       "1  ATL_10_10              30\n",
       "2  ATL_10_11             135\n",
       "3  ATL_10_12             120\n",
       "4  ATL_10_13             105"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the max wind speed for each storm code\n",
    "max_wind_speed = track_file.groupby('storm_code').agg(\n",
    "    max_wind_speed=('knots', 'max')\n",
    ").reset_index()\n",
    "max_wind_speed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_code</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL_10_11</td>\n",
       "      <td>4</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL_10_12</td>\n",
       "      <td>4</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL_10_13</td>\n",
       "      <td>3</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ATL_10_14</td>\n",
       "      <td>1</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  storm_code  category basin\n",
       "0   ATL_10_1         2   ATL\n",
       "2  ATL_10_11         4   ATL\n",
       "3  ATL_10_12         4   ATL\n",
       "4  ATL_10_13         3   ATL\n",
       "5  ATL_10_14         1   ATL"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter by max >= 64 knots\n",
    "storm_filter = max_wind_speed[max_wind_speed[\"max_wind_speed\"] >= 64].copy()\n",
    "\n",
    "# calculate the TC category using the max wind speed\n",
    "storm_filter[\"category\"] = storm_filter[\"max_wind_speed\"].apply(\n",
    "    lambda x: 1 if 64 <= x <= 82 else (2 if 82 < x <= 95 else (3 if 95 < x <= 112 else (4 if 112 < x <= 136 else (5 if x > 136 else 0))))\n",
    ")\n",
    "storm_filter = storm_filter[[\"storm_code\", \"category\"]]\n",
    "\n",
    "# strip the basin from the storm code\n",
    "storm_filter[\"basin\"] = storm_filter[\"storm_code\"].str.extract(r\"^(.*?)_\")\n",
    "\n",
    "storm_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall number of TCs: 982, category 1 or higher number of TCs: 473\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall number of TCs: {len(max_wind_speed)}, category 1 or higher number of TCs: {len(storm_filter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pressure</th>\n",
       "      <th>knots</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>-80.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>-80.2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.2</td>\n",
       "      <td>-80.3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>-80.4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour   lat   lon  pressure  knots storm_code storm_name  \\\n",
       "0  2020     10   20     0  12.1 -80.0         0     15  ATL_20_28       Zeta   \n",
       "1  2020     10   20     6  12.5 -80.1         0     15  ATL_20_28       Zeta   \n",
       "2  2020     10   20    12  12.8 -80.2         0     15  ATL_20_28       Zeta   \n",
       "3  2020     10   20    18  13.2 -80.3         0     15  ATL_20_28       Zeta   \n",
       "4  2020     10   21     0  13.8 -80.4         0     15  ATL_20_28       Zeta   \n",
       "\n",
       "   category basin  \n",
       "0         2   ATL  \n",
       "1         2   ATL  \n",
       "2         2   ATL  \n",
       "3         2   ATL  \n",
       "4         2   ATL  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the trackfile data by the storm filter\n",
    "track_file_filtered = track_file[track_file[\"storm_code\"].isin(storm_filter[\"storm_code\"])]\n",
    "# join the category column by storm code\n",
    "track_file_filtered = pd.merge(track_file_filtered, storm_filter, how='inner', on='storm_code')\n",
    "\n",
    "track_file_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this as a csv file for use in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file_filtered.to_csv('Filtered_Reduced_Trackfile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's focus on the WWLLN dataset. Start by filtering the WWLLN dataset by the storm filter created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter WWLLN dataset by the storm filter\n",
    "locations_WWLLN_filtered = locations_WWLLN[locations_WWLLN[\"storm_code\"].isin(storm_filter[\"storm_code\"])]\n",
    "locations_WWLLN_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the direct distance of each lightning instance from the storm center using a simple triangle calculation. We have the north and east distances from center, so we use the Pythagorean theorem to simply calculate the missing hypotenuse. We also create an indicator for inner core lightning and another for rainband lightning. Inner core is defined as within 100km of storm center, while rainband is defined as between 200-400km of storm center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered['hypotenuse_disance_from_storm_center'] = np.sqrt(locations_WWLLN_filtered['distance_from_storm_center_km_east'] ** 2 +locations_WWLLN['distance_from_storm_center_km_north'] ** 2)\n",
    "locations_WWLLN_filtered[\"inner_core_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n",
    "    lambda x: 1 if x <= 100 else 0\n",
    ")\n",
    "locations_WWLLN_filtered[\"rainband_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n",
    "    lambda x: 1 if (x >= 200 and x <= 400) else 0 # pls check if this works i didnt run it and then delete this comment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this as a txt file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_pl = pl.from_pandas(locations_WWLLN_filtered)\n",
    "locations_WWLLN_filtered_pl.write_csv('Filtered_WWLLN_Locations.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the Data\n",
    "Do we want to put the join here??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
