{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Files - Creating and Cleaning Consolidated Data Files\n",
    "\n",
    "This notebook adds two necessary columns to our data files and combines the individual `.txt` files into two larger `.txt` files. Execution of this notebook will create `Combined_Reduced_Trackfile.txt` and `Combined_WWLLN_Locations.txt`. This notebook should be executed after the `data_file_cleaning.ipynb` notebook.\n",
    "\n",
    "We then perform some post-processing on the data by adding column headers, filtering to tropical cyclones that are category 1 or higher, and calculating the direct distance of each lightning strike to the TC storm center. This will create additional `Filtered_Reduced_Trackfile.csv` and `Filtered_WWLLN_Locations.txt` files for use in analysis. \n",
    "\n",
    "The last part of this notebook joins the trackfile and WWLLN locations together, where we bin the lightning data by 30 minute increments and join to the closest storm center timestamp to get the wind speed and pressure data. This portion will create the `some file` for use in analysis.\n",
    "\n",
    "### Combining Files\n",
    "We use the [Google Drive API](https://developers.google.com/drive/api/guides/about-sdk) to download the files previously uploaded in `data_upload.ipynb` to consolidate the individual files. The first half of the code works if the Google Drive API is already set up (refer to instructions in `data_upload.ipynb`). The code after we create the list of files is not dependent on the Google Drive API.\n",
    "\n",
    "Let's start by installing necessary packages and then importing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import os\n",
    "import polars as pl\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from io import BytesIO\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the function in `data_file_cleaning.ipynb`, we use the following function to authenticate the Google Drive API. This will open a browser to perform the authentication process. \n",
    "\n",
    "Check if a `token.pickle` file already exists before running the following code. If the file exists, it is recommended to delete it before running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=389849867563-4uggnm57nqe52156v32gj1lkosoqpoem.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A37635%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=GyfvJGpr2mtpuRzI6AP8xi3h1k3QIH&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "# Scopes for accessing Google Drive\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Authenticate and create the service object\n",
    "def authenticate_drive_api():\n",
    "    creds = None\n",
    "    # Token file for saving the authentication\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no credentials, perform authentication\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'client_secrets.json', SCOPES)  # Ensure 'credentials.json' is downloaded from Google API Console\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for future use\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Initialize the service object\n",
    "service = authenticate_drive_api()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function grabs the list of all files in a specified folder that are not trashed and stores them into a list. Each file has an ID and name attribute that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(folder_id):\n",
    "    # Query to find files in the specified folder\n",
    "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "    files = []\n",
    "\n",
    "    # List files in the folder and append to list\n",
    "    page_token = None\n",
    "    while True:\n",
    "        response = service.files().list(\n",
    "            q=query,\n",
    "            spaces='drive',\n",
    "            fields='nextPageToken, files(id, name)',\n",
    "            pageToken=page_token\n",
    "        ).execute()\n",
    "\n",
    "        files += response.get('files', [])\n",
    "\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function to find files in the specified folder. The folder ID can be found as the string after the \"folders/\" part of the URL for the Google Drive folder. This will give us a list of files to iterate through for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the folder\n",
    "folder_id = '14idmMBbM5xXZg4b61iINHbBTl2z4yLeD'\n",
    "files = find_files(folder_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split out the tropical cyclone ID and name from each of the files to add as a separate column. We then save the files in the `processed_files` directory for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each file to add cyclone ID and name as columns\n",
    "# Directory to save the processed files locally\n",
    "output_dir = \"processed_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    file_id = file['id']\n",
    "    file_name = file['name']\n",
    "\n",
    "    # Extract the cyclone ID and name from the filename\n",
    "    cyclone_id = '_'.join(file_name.split('_')[:3])\n",
    "    cyclone_name = file_name.split('_')[3]\n",
    "\n",
    "    # Download the file content\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    file_stream = BytesIO()\n",
    "    downloader = MediaIoBaseDownload(file_stream, request)\n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "    file_stream.seek(0)\n",
    "    content = file_stream.read().decode('utf-8')\n",
    "\n",
    "    # Add the cyclone id and name as a new column using Polars\n",
    "    df = pl.read_csv(BytesIO(content.encode('utf-8')),separator='\\t', has_header=False)\n",
    "    df = df.with_columns([\n",
    "    pl.lit(cyclone_id).alias(\"cyclone_id\"),\n",
    "    pl.lit(cyclone_name).alias(\"cyclone_name\")\n",
    "    ])\n",
    "\n",
    "    # Save the modified DataFrame locally\n",
    "    output_file_path = os.path.join(output_dir, file_name)\n",
    "    df.write_csv(output_file_path, separator='\\t',include_header=False)\n",
    "\n",
    "    print(f\"Processed and saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine each of the trackfiles in the `processed_files` folder into one file, and each of the WWLLN location files into one file. This will give us two output files in the `combined_files` folder - `Combined_Reduced_Trackfile.txt` and `Combined_WWLLN_Locations.txt`. We will use these files as the basis for our subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining 992 files for pattern 'Reduced_Trackfile'...\n",
      "Combined file saved: combined_files/Combined_Reduced_Trackfile.txt\n",
      "Combining 994 files for pattern 'WWLLN_Locations'...\n",
      "Combined file saved: combined_files/Combined_WWLLN_Locations.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Directories for processed files and output\n",
    "input_dir = \"processed_files\"\n",
    "output_dir = \"combined_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# File patterns to combine\n",
    "patterns = {\n",
    "    \"Reduced_Trackfile\": os.path.join(input_dir, \"*Reduced_Trackfile*.txt\"),\n",
    "    \"WWLLN_Locations\": os.path.join(input_dir, \"*WWLLN_Locations*.txt\")\n",
    "}\n",
    "\n",
    "# Combine files based on patterns\n",
    "for pattern_name, pattern_path in patterns.items():\n",
    "    combined_content = []\n",
    "    output_file_path = os.path.join(output_dir, f\"Combined_{pattern_name}.txt\")\n",
    "\n",
    "    # Find all matching files\n",
    "    matching_files = glob.glob(pattern_path)\n",
    "    print(f\"Combining {len(matching_files)} files for pattern '{pattern_name}'...\")\n",
    "\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        for file_path in matching_files:\n",
    "            with open(file_path, \"r\") as input_file:\n",
    "                for line in input_file:\n",
    "                    output_file.write(line)\n",
    "\n",
    "    print(f\"Combined file saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Processing\n",
    "In this section we add a column header to the files and filter down to TCs that are category 1 and above. Category 1 is defined using the [Saffir-Simpson Hurricane Wind Scale](https://www.nhc.noaa.gov/aboutsshws.php), where the maximum sustained wind speed is between 64-82 kt. We calculate each TC's category using the Saffir-Simpson Scale and save it in a new column. We then calculate the direct distance of each lightning strike to the storm center and denote it as inner core or rainband. This section outputs `Filtered_Reduced_Trackfile.csv` and `Filtered_WWLLN_Locations.txt` files.\n",
    "\n",
    "Start by importing the necessary libraries and files created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import txt files created earlier\n",
    "# define the path to file below\n",
    "# trackfile_path = \"Combined_Reduced_Trackfile.txt\"\n",
    "# wwlln_path = \"Combined_WWLLN_Locations.txt\"\n",
    "\n",
    "# track_file = pd.read_csv(trackfile_path, sep=\"\\t\")\n",
    "# track_file = track_file.drop(track_file.columns[8], axis=1) # drop the column of zeros\n",
    "\n",
    "\n",
    "#!!!! commented out bc elaine's computer will die so this is for u janice <3\n",
    "chunksize = 100000  # Process 100,000 rows at a time\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    r\"C:\\Users\\user\\Desktop\\25 WI\\Combined_WWLLN_Locations.txt\",\n",
    "    delim_whitespace=True,\n",
    "    chunksize=chunksize\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "locations_WWLLN = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_track_file = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\Filtered_Reduced_Trackfile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add headers to the two dataframes for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file.columns = ['year', 'month', 'day','hour','lat','lon','pressure', 'knots', 'storm_code', 'storm_name']\n",
    "#locations_WWLLN.columns = ['year', 'month', 'day', 'hour', 'min', 'sec','lat','lon','distance_from_storm_center_km_east', 'distance_from_storm_center_km_north', 'storm_code','storm_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we process the trackfile data by creating the list of storm codes that meet the category 1 or higher requirement. We will use this list to filter the wind speed/pressure data as well as the lightning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_code</th>\n",
       "      <th>max_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL_10_10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL_10_11</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL_10_12</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL_10_13</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  storm_code  max_wind_speed\n",
       "0   ATL_10_1              85\n",
       "1  ATL_10_10              30\n",
       "2  ATL_10_11             135\n",
       "3  ATL_10_12             120\n",
       "4  ATL_10_13             105"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the max wind speed for each storm code\n",
    "max_wind_speed = track_file.groupby('storm_code').agg(\n",
    "    max_wind_speed=('knots', 'max')\n",
    ").reset_index()\n",
    "max_wind_speed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_code</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL_10_1</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL_10_11</td>\n",
       "      <td>4</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATL_10_12</td>\n",
       "      <td>4</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATL_10_13</td>\n",
       "      <td>3</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ATL_10_14</td>\n",
       "      <td>1</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  storm_code  category basin\n",
       "0   ATL_10_1         2   ATL\n",
       "2  ATL_10_11         4   ATL\n",
       "3  ATL_10_12         4   ATL\n",
       "4  ATL_10_13         3   ATL\n",
       "5  ATL_10_14         1   ATL"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter by max >= 64 knots\n",
    "storm_filter = max_wind_speed[max_wind_speed[\"max_wind_speed\"] >= 64].copy()\n",
    "\n",
    "# calculate the TC category using the max wind speed\n",
    "storm_filter[\"category\"] = storm_filter[\"max_wind_speed\"].apply(\n",
    "    lambda x: 1 if 64 <= x <= 82 else (2 if 82 < x <= 95 else (3 if 95 < x <= 112 else (4 if 112 < x <= 136 else (5 if x > 136 else 0))))\n",
    ")\n",
    "storm_filter = storm_filter[[\"storm_code\", \"category\"]]\n",
    "\n",
    "# strip the basin from the storm code\n",
    "storm_filter[\"basin\"] = storm_filter[\"storm_code\"].str.extract(r\"^(.*?)_\")\n",
    "\n",
    "storm_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall number of TCs: 982, category 1 or higher number of TCs: 473\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall number of TCs: {len(max_wind_speed)}, category 1 or higher number of TCs: {len(storm_filter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pressure</th>\n",
       "      <th>knots</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>-80.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>-80.2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.2</td>\n",
       "      <td>-80.3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>-80.4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour   lat   lon  pressure  knots storm_code storm_name  \\\n",
       "0  2020     10   20     0  12.1 -80.0         0     15  ATL_20_28       Zeta   \n",
       "1  2020     10   20     6  12.5 -80.1         0     15  ATL_20_28       Zeta   \n",
       "2  2020     10   20    12  12.8 -80.2         0     15  ATL_20_28       Zeta   \n",
       "3  2020     10   20    18  13.2 -80.3         0     15  ATL_20_28       Zeta   \n",
       "4  2020     10   21     0  13.8 -80.4         0     15  ATL_20_28       Zeta   \n",
       "\n",
       "   category basin  \n",
       "0         2   ATL  \n",
       "1         2   ATL  \n",
       "2         2   ATL  \n",
       "3         2   ATL  \n",
       "4         2   ATL  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the trackfile data by the storm filter\n",
    "track_file_filtered = track_file[track_file[\"storm_code\"].isin(storm_filter[\"storm_code\"])]\n",
    "# join the category column by storm code\n",
    "track_file_filtered = pd.merge(track_file_filtered, storm_filter, how='inner', on='storm_code')\n",
    "\n",
    "track_file_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this as a csv file for use in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_file_filtered.to_csv('Filtered_Reduced_Trackfile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's focus on the WWLLN dataset. Start by filtering the WWLLN dataset by the storm filter created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pressure</th>\n",
       "      <th>knots</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>category</th>\n",
       "      <th>basin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>12.5</td>\n",
       "      <td>-80.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>-80.2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.2</td>\n",
       "      <td>-80.3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>-80.4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>ATL_20_28</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>2</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour   lat   lon  pressure  knots storm_code storm_name  \\\n",
       "0  2020     10   20     0  12.1 -80.0         0     15  ATL_20_28       Zeta   \n",
       "1  2020     10   20     6  12.5 -80.1         0     15  ATL_20_28       Zeta   \n",
       "2  2020     10   20    12  12.8 -80.2         0     15  ATL_20_28       Zeta   \n",
       "3  2020     10   20    18  13.2 -80.3         0     15  ATL_20_28       Zeta   \n",
       "4  2020     10   21     0  13.8 -80.4         0     15  ATL_20_28       Zeta   \n",
       "\n",
       "   category basin  \n",
       "0         2   ATL  \n",
       "1         2   ATL  \n",
       "2         2   ATL  \n",
       "3         2   ATL  \n",
       "4         2   ATL  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_track_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Janice from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN.columns = ['year', 'month', 'day', 'hour', 'min', 'sec','lat','lon','distance_from_storm_center_km_east', 'distance_from_storm_center_km_north', 'storm_code','storm_name']  # Replace with actual names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>sec</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>distance_from_storm_center_km_east</th>\n",
       "      <th>distance_from_storm_center_km_north</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>735356</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>13.4587</td>\n",
       "      <td>-96.2292</td>\n",
       "      <td>234.752</td>\n",
       "      <td>228.917</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735357</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>1.0418</td>\n",
       "      <td>13.3158</td>\n",
       "      <td>-96.3223</td>\n",
       "      <td>224.818</td>\n",
       "      <td>213.027</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735358</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>9.4671</td>\n",
       "      <td>12.9812</td>\n",
       "      <td>-88.4452</td>\n",
       "      <td>1078.570</td>\n",
       "      <td>175.821</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735359</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>9.4150</td>\n",
       "      <td>13.0012</td>\n",
       "      <td>-88.4697</td>\n",
       "      <td>1075.820</td>\n",
       "      <td>178.045</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735360</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>9.4671</td>\n",
       "      <td>12.9800</td>\n",
       "      <td>-88.4276</td>\n",
       "      <td>1080.480</td>\n",
       "      <td>175.688</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  day  hour  min     sec      lat      lon  \\\n",
       "735356  2016     10   22     5   56  0.6103  13.4587 -96.2292   \n",
       "735357  2016     10   22     5   56  1.0418  13.3158 -96.3223   \n",
       "735358  2016     10   22     5   56  9.4671  12.9812 -88.4452   \n",
       "735359  2016     10   22     5   56  9.4150  13.0012 -88.4697   \n",
       "735360  2016     10   22     5   56  9.4671  12.9800 -88.4276   \n",
       "\n",
       "        distance_from_storm_center_km_east  \\\n",
       "735356                             234.752   \n",
       "735357                             224.818   \n",
       "735358                            1078.570   \n",
       "735359                            1075.820   \n",
       "735360                            1080.480   \n",
       "\n",
       "        distance_from_storm_center_km_north  storm_code storm_name  \n",
       "735356                              228.917  EPAC_16_20    Seymour  \n",
       "735357                              213.027  EPAC_16_20    Seymour  \n",
       "735358                              175.821  EPAC_16_20    Seymour  \n",
       "735359                              178.045  EPAC_16_20    Seymour  \n",
       "735360                              175.688  EPAC_16_20    Seymour  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter WWLLN dataset by the storm filter\n",
    "locations_WWLLN_filtered = locations_WWLLN[locations_WWLLN[\"storm_code\"].isin(filtered_track_file[\"storm_code\"])]\n",
    "locations_WWLLN_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the direct distance of each lightning instance from the storm center using a simple triangle calculation. We have the north and east distances from center, so we use the Pythagorean theorem to simply calculate the missing hypotenuse. We also create an indicator for inner core lightning and another for rainband lightning. Inner core is defined as within 100km of storm center, while rainband is defined as between 200-400km of storm center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143111669, 12)\n"
     ]
    }
   ],
   "source": [
    "print(locations_WWLLN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77535990, 12)\n"
     ]
    }
   ],
   "source": [
    "print(locations_WWLLN_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26800\\3001479597.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  locations_WWLLN_filtered['hypotenuse_disance_from_storm_center'] = np.sqrt(locations_WWLLN_filtered['distance_from_storm_center_km_east'] ** 2 +locations_WWLLN['distance_from_storm_center_km_north'] ** 2)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26800\\3001479597.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  locations_WWLLN_filtered[\"inner_core_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26800\\3001479597.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  locations_WWLLN_filtered[\"rainband_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n"
     ]
    }
   ],
   "source": [
    "locations_WWLLN_filtered['hypotenuse_disance_from_storm_center'] = np.sqrt(locations_WWLLN_filtered['distance_from_storm_center_km_east'] ** 2 +locations_WWLLN['distance_from_storm_center_km_north'] ** 2)\n",
    "locations_WWLLN_filtered[\"inner_core_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n",
    "    lambda x: 1 if x <= 100 else 0\n",
    ")\n",
    "locations_WWLLN_filtered[\"rainband_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n",
    "     lambda x: 1 if (x >= 200 and x <= 400) else 0 # pls check if this works i didnt run it and then delete this comment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15952\\933412928.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  locations_WWLLN_filtered[\"rainband_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n"
     ]
    }
   ],
   "source": [
    "locations_WWLLN_filtered[\"rainband_ind\"] = locations_WWLLN_filtered[\"hypotenuse_disance_from_storm_center\"].apply(\n",
    "     lambda x: 1 if (x >= 200 and x <= 400) else 0 # pls check if this works i didnt run it and then delete this comment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered.to_csv(\"Filtered_WWLLN_Locations.txt\", sep=' ', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\user\\\\Desktop\\\\25 WI\\\\Filtered_WWLLN_Locations.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m chunksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m  \u001b[38;5;66;03m# Process 100,000 rows at a time\u001b[39;00m\n\u001b[0;32m      2\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m25 WI\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFiltered_WWLLN_Locations.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelim_whitespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m:\n\u001b[0;32m      9\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[0;32m     11\u001b[0m locations_WWLLN_filtered_ \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(chunks, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\drg\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\drg\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\drg\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\drg\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\drg\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\drg\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\drg\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\user\\\\Desktop\\\\25 WI\\\\Filtered_WWLLN_Locations.txt'"
     ]
    }
   ],
   "source": [
    "chunksize = 100000  # Process 100,000 rows at a time\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    r\"C:\\Users\\user\\Desktop\\25 WI\\Filtered_WWLLN_Locations.txt\",\n",
    "    delim_whitespace=True,\n",
    "    chunksize=chunksize\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "locations_WWLLN_filtered_ = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>sec</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>distance_from_storm_center_km_east</th>\n",
       "      <th>distance_from_storm_center_km_north</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>hypotenuse_disance_from_storm_center</th>\n",
       "      <th>inner_core_ind</th>\n",
       "      <th>rainband_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>13.4587</td>\n",
       "      <td>-96.2292</td>\n",
       "      <td>234.752</td>\n",
       "      <td>228.917</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>327.889455</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>1.0418</td>\n",
       "      <td>13.3158</td>\n",
       "      <td>-96.3223</td>\n",
       "      <td>224.818</td>\n",
       "      <td>213.027</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>309.715411</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>9.4671</td>\n",
       "      <td>12.9812</td>\n",
       "      <td>-88.4452</td>\n",
       "      <td>1078.570</td>\n",
       "      <td>175.821</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>1092.806602</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>9.4150</td>\n",
       "      <td>13.0012</td>\n",
       "      <td>-88.4697</td>\n",
       "      <td>1075.820</td>\n",
       "      <td>178.045</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>1090.453435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>9.4671</td>\n",
       "      <td>12.9800</td>\n",
       "      <td>-88.4276</td>\n",
       "      <td>1080.480</td>\n",
       "      <td>175.688</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>1094.670409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour  min     sec      lat      lon  \\\n",
       "0  2016     10   22     5   56  0.6103  13.4587 -96.2292   \n",
       "1  2016     10   22     5   56  1.0418  13.3158 -96.3223   \n",
       "2  2016     10   22     5   56  9.4671  12.9812 -88.4452   \n",
       "3  2016     10   22     5   56  9.4150  13.0012 -88.4697   \n",
       "4  2016     10   22     5   56  9.4671  12.9800 -88.4276   \n",
       "\n",
       "   distance_from_storm_center_km_east  distance_from_storm_center_km_north  \\\n",
       "0                             234.752                              228.917   \n",
       "1                             224.818                              213.027   \n",
       "2                            1078.570                              175.821   \n",
       "3                            1075.820                              178.045   \n",
       "4                            1080.480                              175.688   \n",
       "\n",
       "   storm_code storm_name  hypotenuse_disance_from_storm_center  \\\n",
       "0  EPAC_16_20    Seymour                            327.889455   \n",
       "1  EPAC_16_20    Seymour                            309.715411   \n",
       "2  EPAC_16_20    Seymour                           1092.806602   \n",
       "3  EPAC_16_20    Seymour                           1090.453435   \n",
       "4  EPAC_16_20    Seymour                           1094.670409   \n",
       "\n",
       "   inner_core_ind  rainband_ind  \n",
       "0               0             1  \n",
       "1               0             1  \n",
       "2               0             0  \n",
       "3               0             0  \n",
       "4               0             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_WWLLN_filtered_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_final = locations_WWLLN_filtered_[\n",
    "    ~((locations_WWLLN_filtered_[\"rainband_ind\"] == 0) & (locations_WWLLN_filtered_[\"inner_core_ind\"] == 0))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_final.to_csv(\"Filtered_WWLLN_Locations_final.txt\", sep=' ', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2611951, 15)\n"
     ]
    }
   ],
   "source": [
    "locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_final[locations_WWLLN_filtered_final['inner_core_ind'] == 1]\n",
    "print(locations_WWLLN_filtered_innercore.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10146702, 15)\n"
     ]
    }
   ],
   "source": [
    "locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_final[locations_WWLLN_filtered_final['rainband_ind'] == 1]\n",
    "print(locations_WWLLN_filtered_rainband.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_innercore.to_csv(\"WWLLN_innercore.csv\", index=False)\n",
    "locations_WWLLN_filtered_rainband.to_csv(\"WWLLN_rainband.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_innercore = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_innercore_original.csv\")\n",
    "locations_WWLLN_filtered_rainband = pd.read_csv(r\"C:\\Users\\user\\Desktop\\25 WI\\WWLLN_rainband_original.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this as a txt file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>sec</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>distance_from_storm_center_km_east</th>\n",
       "      <th>distance_from_storm_center_km_north</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>hypotenuse_disance_from_storm_center</th>\n",
       "      <th>inner_core_ind</th>\n",
       "      <th>rainband_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>10.5194</td>\n",
       "      <td>12.2078</td>\n",
       "      <td>-99.0228</td>\n",
       "      <td>-51.2569</td>\n",
       "      <td>85.3196</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>99.532427</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>58.6195</td>\n",
       "      <td>11.2449</td>\n",
       "      <td>-98.8947</td>\n",
       "      <td>-17.5788</td>\n",
       "      <td>-27.2301</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>32.411303</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>58.6161</td>\n",
       "      <td>11.1921</td>\n",
       "      <td>-98.8502</td>\n",
       "      <td>-12.7280</td>\n",
       "      <td>-33.1012</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>35.463945</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>54</td>\n",
       "      <td>41.9061</td>\n",
       "      <td>12.1096</td>\n",
       "      <td>-99.3377</td>\n",
       "      <td>-65.6873</td>\n",
       "      <td>68.9202</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>95.209324</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>47.8987</td>\n",
       "      <td>11.5981</td>\n",
       "      <td>-99.5928</td>\n",
       "      <td>-86.9552</td>\n",
       "      <td>10.2015</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>87.551570</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour  min      sec      lat      lon  \\\n",
       "0  2016     10   22     6   53  10.5194  12.2078 -99.0228   \n",
       "1  2016     10   22     7   53  58.6195  11.2449 -98.8947   \n",
       "2  2016     10   22     7   53  58.6161  11.1921 -98.8502   \n",
       "3  2016     10   22     7   54  41.9061  12.1096 -99.3377   \n",
       "4  2016     10   22     8   13  47.8987  11.5981 -99.5928   \n",
       "\n",
       "   distance_from_storm_center_km_east  distance_from_storm_center_km_north  \\\n",
       "0                            -51.2569                              85.3196   \n",
       "1                            -17.5788                             -27.2301   \n",
       "2                            -12.7280                             -33.1012   \n",
       "3                            -65.6873                              68.9202   \n",
       "4                            -86.9552                              10.2015   \n",
       "\n",
       "   storm_code storm_name  hypotenuse_disance_from_storm_center  \\\n",
       "0  EPAC_16_20    Seymour                             99.532427   \n",
       "1  EPAC_16_20    Seymour                             32.411303   \n",
       "2  EPAC_16_20    Seymour                             35.463945   \n",
       "3  EPAC_16_20    Seymour                             95.209324   \n",
       "4  EPAC_16_20    Seymour                             87.551570   \n",
       "\n",
       "   inner_core_ind  rainband_ind  \n",
       "0               1             0  \n",
       "1               1             0  \n",
       "2               1             0  \n",
       "3               1             0  \n",
       "4               1             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_WWLLN_filtered_innercore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21336\\2364616314.py:19: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_innercore.groupby('storm_code').apply(add_time_bin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month  day  hour  min      sec      lat      lon  \\\n",
      "0  2016     10   22     6   53  10.5194  12.2078 -99.0228   \n",
      "1  2016     10   22     7   53  58.6195  11.2449 -98.8947   \n",
      "2  2016     10   22     7   53  58.6161  11.1921 -98.8502   \n",
      "3  2016     10   22     7   54  41.9061  12.1096 -99.3377   \n",
      "4  2016     10   22     8   13  47.8987  11.5981 -99.5928   \n",
      "\n",
      "   distance_from_storm_center_km_east  distance_from_storm_center_km_north  \\\n",
      "0                            -51.2569                              85.3196   \n",
      "1                            -17.5788                             -27.2301   \n",
      "2                            -12.7280                             -33.1012   \n",
      "3                            -65.6873                              68.9202   \n",
      "4                            -86.9552                              10.2015   \n",
      "\n",
      "   storm_code storm_name  hypotenuse_disance_from_storm_center  \\\n",
      "0  EPAC_16_20    Seymour                             99.532427   \n",
      "1  EPAC_16_20    Seymour                             32.411303   \n",
      "2  EPAC_16_20    Seymour                             35.463945   \n",
      "3  EPAC_16_20    Seymour                             95.209324   \n",
      "4  EPAC_16_20    Seymour                             87.551570   \n",
      "\n",
      "   inner_core_ind  rainband_ind                   datetime            time_bin  \n",
      "0               1             0 2016-10-22 06:53:10.519400 2016-10-22 06:30:00  \n",
      "1               1             0 2016-10-22 07:53:58.619500 2016-10-22 07:30:00  \n",
      "2               1             0 2016-10-22 07:53:58.616100 2016-10-22 07:30:00  \n",
      "3               1             0 2016-10-22 07:54:41.906100 2016-10-22 07:30:00  \n",
      "4               1             0 2016-10-22 08:13:47.898700 2016-10-22 08:00:00  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming your columns are 'year', 'month', 'day', 'hour', 'minute', 'second', and 'storm_code'\n",
    "# Create a datetime column from the existing columns\n",
    "#locations_WWLLN_filtered_innercore['datetime'] = pd.to_datetime(locations_WWLLN_filtered_innercore[['year', 'month', 'day', 'hour', 'min', 'sec']])\n",
    "locations_WWLLN_filtered_innercore['sec'] = locations_WWLLN_filtered_innercore['sec'].apply(lambda x: 0 if x == 60 else x)\n",
    "\n",
    "locations_WWLLN_filtered_innercore['datetime'] = pd.to_datetime(locations_WWLLN_filtered_innercore['year'].astype(str) + '-' +\n",
    "                                 locations_WWLLN_filtered_innercore['month'].astype(str).str.zfill(2) + '-' +\n",
    "                                 locations_WWLLN_filtered_innercore['day'].astype(str).str.zfill(2) + ' ' +\n",
    "                                 locations_WWLLN_filtered_innercore['hour'].astype(str).str.zfill(2) + ':' +\n",
    "                                 locations_WWLLN_filtered_innercore['min'].astype(str).str.zfill(2) + ':' +\n",
    "                                 locations_WWLLN_filtered_innercore['sec'].astype(str).str.zfill(2))\n",
    "\n",
    "# Define a function to apply the 30-minute binning for each storm_code group\n",
    "def add_time_bin(group):\n",
    "    group['time_bin'] = group['datetime'].dt.floor('30T')\n",
    "    return group\n",
    "\n",
    "# Group by storm_code and apply the binning function\n",
    "locations_WWLLN_filtered_innercore = locations_WWLLN_filtered_innercore.groupby('storm_code').apply(add_time_bin)\n",
    "\n",
    "# Print the resulting DataFrame with the new 'time_bin' column\n",
    "print(locations_WWLLN_filtered_innercore.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27748\\3369465188.py:16: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_rainband.groupby('storm_code').apply(add_time_bin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>sec</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>distance_from_storm_center_km_east</th>\n",
       "      <th>distance_from_storm_center_km_north</th>\n",
       "      <th>storm_code</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>hypotenuse_disance_from_storm_center</th>\n",
       "      <th>inner_core_ind</th>\n",
       "      <th>rainband_ind</th>\n",
       "      <th>datetime</th>\n",
       "      <th>time_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>13.4587</td>\n",
       "      <td>-96.2292</td>\n",
       "      <td>234.752</td>\n",
       "      <td>228.917</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>327.889455</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-22 05:56:00.610300</td>\n",
       "      <td>2016-10-22 05:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>1.0418</td>\n",
       "      <td>13.3158</td>\n",
       "      <td>-96.3223</td>\n",
       "      <td>224.818</td>\n",
       "      <td>213.027</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>309.715411</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-22 05:56:01.041800</td>\n",
       "      <td>2016-10-22 05:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>54.1845</td>\n",
       "      <td>13.2070</td>\n",
       "      <td>-96.1878</td>\n",
       "      <td>239.479</td>\n",
       "      <td>200.929</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>312.606229</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-22 05:56:54.184500</td>\n",
       "      <td>2016-10-22 05:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>54.2637</td>\n",
       "      <td>13.3043</td>\n",
       "      <td>-96.2315</td>\n",
       "      <td>234.654</td>\n",
       "      <td>211.748</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>316.069162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-22 05:56:54.263700</td>\n",
       "      <td>2016-10-22 05:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>54.3229</td>\n",
       "      <td>13.2674</td>\n",
       "      <td>-96.2462</td>\n",
       "      <td>233.099</td>\n",
       "      <td>207.645</td>\n",
       "      <td>EPAC_16_20</td>\n",
       "      <td>Seymour</td>\n",
       "      <td>312.172372</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-22 05:56:54.322900</td>\n",
       "      <td>2016-10-22 05:30:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour  min      sec      lat      lon  \\\n",
       "0  2016     10   22     5   56   0.6103  13.4587 -96.2292   \n",
       "1  2016     10   22     5   56   1.0418  13.3158 -96.3223   \n",
       "2  2016     10   22     5   56  54.1845  13.2070 -96.1878   \n",
       "3  2016     10   22     5   56  54.2637  13.3043 -96.2315   \n",
       "4  2016     10   22     5   56  54.3229  13.2674 -96.2462   \n",
       "\n",
       "   distance_from_storm_center_km_east  distance_from_storm_center_km_north  \\\n",
       "0                             234.752                              228.917   \n",
       "1                             224.818                              213.027   \n",
       "2                             239.479                              200.929   \n",
       "3                             234.654                              211.748   \n",
       "4                             233.099                              207.645   \n",
       "\n",
       "   storm_code storm_name  hypotenuse_disance_from_storm_center  \\\n",
       "0  EPAC_16_20    Seymour                            327.889455   \n",
       "1  EPAC_16_20    Seymour                            309.715411   \n",
       "2  EPAC_16_20    Seymour                            312.606229   \n",
       "3  EPAC_16_20    Seymour                            316.069162   \n",
       "4  EPAC_16_20    Seymour                            312.172372   \n",
       "\n",
       "   inner_core_ind  rainband_ind                   datetime            time_bin  \n",
       "0               0             1 2016-10-22 05:56:00.610300 2016-10-22 05:30:00  \n",
       "1               0             1 2016-10-22 05:56:01.041800 2016-10-22 05:30:00  \n",
       "2               0             1 2016-10-22 05:56:54.184500 2016-10-22 05:30:00  \n",
       "3               0             1 2016-10-22 05:56:54.263700 2016-10-22 05:30:00  \n",
       "4               0             1 2016-10-22 05:56:54.322900 2016-10-22 05:30:00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_WWLLN_filtered_rainband['sec'] = locations_WWLLN_filtered_rainband['sec'].apply(lambda x: 0 if x == 60 else x)\n",
    "\n",
    "locations_WWLLN_filtered_rainband['datetime'] = pd.to_datetime(locations_WWLLN_filtered_rainband['year'].astype(str) + '-' +\n",
    "                                 locations_WWLLN_filtered_rainband['month'].astype(str).str.zfill(2) + '-' +\n",
    "                                 locations_WWLLN_filtered_rainband['day'].astype(str).str.zfill(2) + ' ' +\n",
    "                                 locations_WWLLN_filtered_rainband['hour'].astype(str).str.zfill(2) + ':' +\n",
    "                                 locations_WWLLN_filtered_rainband['min'].astype(str).str.zfill(2) + ':' +\n",
    "                                 locations_WWLLN_filtered_rainband['sec'].astype(str).str.zfill(2))\n",
    "\n",
    "# Define a function to apply the 30-minute binning for each storm_code group\n",
    "def add_time_bin(group):\n",
    "    group['time_bin'] = group['datetime'].dt.floor('30T')\n",
    "    return group\n",
    "\n",
    "# Group by storm_code and apply the binning function\n",
    "locations_WWLLN_filtered_rainband = locations_WWLLN_filtered_rainband.groupby('storm_code').apply(add_time_bin)\n",
    "\n",
    "# Print the resulting DataFrame with the new 'time_bin' column\n",
    "locations_WWLLN_filtered_rainband.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_innercore_grouped = locations_WWLLN_filtered_innercore.groupby(['storm_code', 'time_bin'])\n",
    "locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_grouped.size().reset_index(name='lightining_count')\n",
    "\n",
    "locations_WWLLN_filtered_innercore_timebin = locations_WWLLN_filtered_innercore_timebin.sort_values(by = ['storm_code', 'time_bin'])\n",
    "\n",
    "\n",
    "locations_WWLLN_filtered_innercore.to_csv(\"WWLLN_innercore_w_time.csv\", index=False)\n",
    "locations_WWLLN_filtered_innercore_timebin.to_csv(\"WWLLN_innercore_timebin_count.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_WWLLN_filtered_rainband_grouped = locations_WWLLN_filtered_rainband.groupby(['storm_code', 'time_bin'])\n",
    "locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_grouped.size().reset_index(name='lightining_count')\n",
    "\n",
    "locations_WWLLN_filtered_rainband_timebin = locations_WWLLN_filtered_rainband_timebin.sort_values(by = ['storm_code', 'time_bin'])\n",
    "\n",
    "locations_WWLLN_filtered_rainband_timebin.to_csv(\"WWLLN_rainband_timebin_count.csv\", index=False)\n",
    "locations_WWLLN_filtered_rainband.to_csv(\"WWLLN_rainband_w_time.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the Data\n",
    "Do we want to put the join here??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
